# 使用NumPy实现CNN（以MNIST为例）

## 要求

使用Python语言，不使用深度学习库，在MNIST或CIFAR-10数据集上训练一个5层的神经网络。包括全连接层，最大池化层。使用ReLU作为其激活函数。

## 分析

考虑到MNIST数据集规模较小，故选用MNIST作为数据集。

使用NumPy作为矩阵运算的库。将常见的功能封装为一个类，方便调用。

在CPU上运行比较慢，所以优先采用较小的卷积核。权重参数初始化方式为Kaiming He提出的方法。

我设计出的CNN架构如下：（图像输入尺寸：(28, 28, 1)）

| 层数 |    层的名称     |            层的参数            |
| :--: | :-------------: | :----------------------------: |
|  1   | **卷积层conv1** | 3*3卷积核，步长1，卷积核数量5  |
|  1   |    激活函数     |              ReLU              |
|  1   |      池化       |          Max Pooling           |
|  2   | **卷积层conv2** | 3*3卷积核，步长2，卷积核数量10 |
|  2   |    激活函数     |              ReLU              |
|  3   | **卷积层conv3** | 1*1卷积核，步长1，卷积核数量10 |
|  3   |    激活函数     |              ReLU              |
|  3   |      池化       |          Max Pooling           |
|  4   | **全连接层fc4** |          30个输出结点          |
|  4   |    激活函数     |              ReLU              |
|  5   | **全连接层fc5** |          10个输出结点          |
|  5   |      分类       |            softmax             |

## 代码设计

### 组织结构

使用`.forward()`函数作为正向传播，使用`.backward()`函数作为反向传播求梯度，使用`.update()`函数作为更新权重参数的方法。

我设计了5种封装的层：`Conv2D`、`FullyConnect`、`MaxPooling`、`ReLU,`、`Softmax`

以及1种模型类：Model_5LayersCNN（封装了上述的CNN架构）

5种层分别对应于5个`.py`文件，并封装为一个叫`npnet`的库，方便使用。

### 细节设计



# 代码

见附件，或者我的[代码仓库](https://github.com/Karbo123/npnet)
